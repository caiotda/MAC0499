{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf054dbd-e76c-46a0-b7cd-4fe2e2626566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..') # Allows imports from parent folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2ff45e-7bfc-49c0-9e5d-ecdb5e6c698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from model import NERClassifier\n",
    "from preprocess_dataset import NERDataset \n",
    "from trainner import Trainner\n",
    "from transformers import BertTokenizerFast\n",
    "from preprocess_dataset import remove_empty_entries\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b8d62-1209-44cf-af79-0245484a09d5",
   "metadata": {},
   "source": [
    "## Checkpoints to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055f8e3f-1de3-476c-8b0b-8facfbccbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_base_cased    = 'neuralmind/bert-base-portuguese-cased'\n",
    "pt_large_cased   = 'neuralmind/bert-large-portuguese-cased'\n",
    "en_base_uncased = 'bert-base-uncased'\n",
    "en_large_uncased = 'bert-large-uncased'\n",
    "en_base_cased = 'bert-base-cased'\n",
    "en_large_cased = 'bert-large-cased'\n",
    "\n",
    "pt = [pt_base_cased, pt_large_cased]\n",
    "en = [en_base_cased, en_large_cased, en_base_uncased, en_large_uncased]\n",
    "cased = [pt_base_cased, en_base_cased, pt_large_cased, en_large_cased]\n",
    "uncased = [en_base_uncased, en_large_uncased] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "251fd139-3807-4ea0-b50b-5669bb794bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [pt, en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d2acaf-2d08-485a-b7d9-3d2d29eb7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_models = [check for checkpoints in models for check in checkpoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e1eaec4-a9a0-4fd2-8521-b0a3bb93c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "portuguese_flat = list(filter(lambda x: x.find('/') != -1, flat_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ebc270-435d-4976-b771-7ead4f28ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_flat = list(filter(lambda x: x.find('/') == -1, flat_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c938b54f-6eab-409a-95f2-76d4b66b800e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f906c54b-df78-4a3b-9524-18612729bcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neuralmind/bert-base-portuguese-cased',\n",
       " 'neuralmind/bert-large-portuguese-cased']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portuguese_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71a9bd00-2351-42bf-b513-2f205a5ef25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neuralmind/bert-base-portuguese-cased',\n",
       " 'neuralmind/bert-large-portuguese-cased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eabcb836-7e5d-4dd0-9b76-899395fa626c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd8865-974c-42a1-8f4c-36e38f61fd70",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aed8e0c-d15f-4e5d-b19b-848a24b39997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lener_br (/home/caiotulio/.cache/huggingface/datasets/lener_br/lener_br/1.0.0/4a8c97e6813b5c2d85a50faf0a3e6c24ea82f4a9044e6e9e8b24997d27399382)\n",
      "Loading cached processed dataset at /home/caiotulio/.cache/huggingface/datasets/lener_br/lener_br/1.0.0/4a8c97e6813b5c2d85a50faf0a3e6c24ea82f4a9044e6e9e8b24997d27399382/cache-5e59bc59f25f3d7f.arrow\n",
      "Loading cached processed dataset at /home/caiotulio/.cache/huggingface/datasets/lener_br/lener_br/1.0.0/4a8c97e6813b5c2d85a50faf0a3e6c24ea82f4a9044e6e9e8b24997d27399382/cache-8d0457760cd67ee6.arrow\n",
      "Loading cached processed dataset at /home/caiotulio/.cache/huggingface/datasets/lener_br/lener_br/1.0.0/4a8c97e6813b5c2d85a50faf0a3e6c24ea82f4a9044e6e9e8b24997d27399382/cache-74e841c1c151996a.arrow\n"
     ]
    }
   ],
   "source": [
    "data = \"lener_br\"\n",
    "dataset = load_dataset(data)\n",
    "dataset = remove_empty_entries(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a32f69-513f-4a31-9276-c97095c2bb39",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68817378-4e18-4cd3-82fd-e182148bbb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "LEARNING_RATE=3e-4\n",
    "n_labels = 13\n",
    "BATCH_SIZE=8\n",
    "shuffle=True\n",
    "NUM_EPOCHS=1\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d81e7a-32ad-4b1f-a3cc-5bb8d688050d",
   "metadata": {},
   "source": [
    "## Training different checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d2477b-d699-40b3-887e-414f0d1e15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluator import Evaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6b40c4a-4fa5-4948-90b8-17604287fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(checkpoints):\n",
    "\n",
    "    data = {\"f1_t\":[], \"f1_e\":[], \"loss_t\": [], \"loss_e\": []}\n",
    "\n",
    "    for idx, checkpoint in enumerate(checkpoints):\n",
    "        print(f\"Progresso: {idx+1}/{len(checkpoints)}\")\n",
    "        print(f\"------Iniciando treino para o checkpoint {checkpoint}---------\")\n",
    "\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(checkpoint)\n",
    "        print(\"Tokenizer carregado!\")\n",
    "        pytorch_dataset_train = NERDataset(data=dataset['train'], max_len=MAX_LEN, tokenizer=tokenizer)\n",
    "        pytorch_dataset_test = NERDataset(data=dataset['test'], max_len=MAX_LEN, tokenizer=tokenizer)\n",
    "        \n",
    "        loader_t = DataLoader(pytorch_dataset_train, batch_size=BATCH_SIZE, shuffle=shuffle)\n",
    "        loader_e = DataLoader(pytorch_dataset_test, batch_size=BATCH_SIZE, shuffle=shuffle)\n",
    "        print(\"Dataloader carregado!\")\n",
    "\n",
    "        model = NERClassifier(n_labels=n_labels, checkpoint=checkpoint)\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
    "        evaluator = Evaluator(loader_e, model, device)\n",
    "        tr = Trainner(device,\\\n",
    "                      loader_t,\\\n",
    "                      model,\\\n",
    "                      optimizer,\\\n",
    "                      max_len=MAX_LEN,\\\n",
    "                      num_examples=len(pytorch_dataset_train),\\\n",
    "                      num_epochs=NUM_EPOCHS,\\\n",
    "                      evaluator=evaluator)\n",
    "        print(\"Trainner carregado!\")\n",
    "        loss_t, loss_e, f1_e, f1_t = tr.train()\n",
    "        print(f\"Treino finalizado para o checkpoint {checkpoint}\\n\" + \\\n",
    "              f\"loss_t:{loss_t}, loss_e:{loss_e}, f1_e:{f1_e}, f1_t:{f1_t}\")\n",
    "        data[\"f1_t\"].append(f1_t[0]) # We return the f1 score for all epochs. Since we're using \n",
    "        data[\"f1_e\"].append(f1_e[0]) # num_epochs=1, we'll just take the first item.\n",
    "        data[\"loss_t\"].append(loss_t[0])\n",
    "        data[\"loss_e\"].append(loss_e[0])\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230285c9-5a95-469d-93bd-38d790c2b5c6",
   "metadata": {},
   "source": [
    "## Comparing performance por portuguese checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87c007c5-7bc2-4f02-9c2d-4811b9daa93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progresso: 1/2\n",
      "------Iniciando treino para o checkpoint neuralmind/bert-base-portuguese-cased---------\n",
      "Tokenizer carregado!\n",
      "Dataloader carregado!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainner carregado!\n",
      "Começando treino! Essa função retorna a media de f1 e loss em cada epoch de treino e avaliação\n",
      "----------Começando treino da epoch nº 1\n",
      "Treinando em cuda\n",
      "Iteração 0 -------- Loss: 2.4702069759368896 f1 nas ultimas 100 iterações: 0.002755956898358687 ------ Progresso: 0.00%.\n",
      "Iteração 100 -------- Loss: 0.4600450396537781 f1 nas ultimas 100 iterações: 0.8056355955265156 ------ Progresso: 10.21%.\n",
      "Iteração 200 -------- Loss: 0.4597471356391907 f1 nas ultimas 100 iterações: 0.8029780273614623 ------ Progresso: 20.43%.\n",
      "Iteração 300 -------- Loss: 0.4899894893169403 f1 nas ultimas 100 iterações: 0.8281529374266171 ------ Progresso: 30.64%.\n",
      "Iteração 400 -------- Loss: 0.6882858872413635 f1 nas ultimas 100 iterações: 0.8229732041326381 ------ Progresso: 40.86%.\n",
      "Iteração 500 -------- Loss: 1.063020944595337 f1 nas ultimas 100 iterações: 0.8148661588121638 ------ Progresso: 51.07%.\n",
      "Iteração 600 -------- Loss: 1.0118082761764526 f1 nas ultimas 100 iterações: 0.8240364039683132 ------ Progresso: 61.29%.\n",
      "Iteração 700 -------- Loss: 0.4150064289569855 f1 nas ultimas 100 iterações: 0.8135465308827134 ------ Progresso: 71.50%.\n",
      "Iteração 800 -------- Loss: 0.39652249217033386 f1 nas ultimas 100 iterações: 0.7932999000025701 ------ Progresso: 81.72%.\n",
      "Iteração 900 -------- Loss: 0.38699498772621155 f1 nas ultimas 100 iterações: 0.8356521482830652 ------ Progresso: 91.93%.\n",
      "----------Fim do treino. Iniciando avaliação!\n",
      "Iteração 100 -------- Loss: 0.7138868570327759 f1 nas ultimas 100 iterações: 0.8587977809325962 ------ Progresso: 57.47%.\n",
      "-------Fim da epoch nº 1\n",
      "Dados de treino: Loss media da epoch: 0.660471464841188; f1 medio da epoch: 0.8158998201051125\n",
      "Dados de avaliação: Loss media da epoch: 0.5667790524747179; f1 medio da epoch: 0.8392532140148848\n",
      "FIM DO TREINO! f1 media de treino ao fim de 1 epochs: 0.8158998201051125\n",
      "Treino finalizado para o checkpoint neuralmind/bert-base-portuguese-cased\n",
      "loss_t:[0.660471464841188], loss_e:[0.5667790524747179], f1_e:[0.8392532140148848], f1_t:[0.8158998201051125]\n",
      "Progresso: 2/2\n",
      "------Iniciando treino para o checkpoint neuralmind/bert-large-portuguese-cased---------\n",
      "Tokenizer carregado!\n",
      "Dataloader carregado!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-large-portuguese-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-large-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainner carregado!\n",
      "Começando treino! Essa função retorna a media de f1 e loss em cada epoch de treino e avaliação\n",
      "----------Começando treino da epoch nº 1\n",
      "Treinando em cuda\n",
      "Iteração 0 -------- Loss: 2.7701570987701416 f1 nas ultimas 100 iterações: 0.0004972591284957725 ------ Progresso: 0.00%.\n",
      "Iteração 100 -------- Loss: 1.189145565032959 f1 nas ultimas 100 iterações: 0.769894145387029 ------ Progresso: 10.21%.\n",
      "Iteração 200 -------- Loss: 0.5390634536743164 f1 nas ultimas 100 iterações: 0.8167756595651062 ------ Progresso: 20.43%.\n",
      "Iteração 300 -------- Loss: 0.29373303055763245 f1 nas ultimas 100 iterações: 0.8186484467385783 ------ Progresso: 30.64%.\n",
      "Iteração 400 -------- Loss: 0.8086936473846436 f1 nas ultimas 100 iterações: 0.8297792277821973 ------ Progresso: 40.86%.\n",
      "Iteração 500 -------- Loss: 0.47747480869293213 f1 nas ultimas 100 iterações: 0.809640515387076 ------ Progresso: 51.07%.\n",
      "Iteração 600 -------- Loss: 0.5507462024688721 f1 nas ultimas 100 iterações: 0.8188105691635679 ------ Progresso: 61.29%.\n",
      "Iteração 700 -------- Loss: 1.2006900310516357 f1 nas ultimas 100 iterações: 0.823036886055045 ------ Progresso: 71.50%.\n",
      "Iteração 800 -------- Loss: 0.1642540693283081 f1 nas ultimas 100 iterações: 0.8172202780264441 ------ Progresso: 81.72%.\n",
      "Iteração 900 -------- Loss: 0.6595617532730103 f1 nas ultimas 100 iterações: 0.8128808114178401 ------ Progresso: 91.93%.\n",
      "----------Fim do treino. Iniciando avaliação!\n",
      "Iteração 100 -------- Loss: 0.7237746119499207 f1 nas ultimas 100 iterações: 0.8370161433239798 ------ Progresso: 57.47%.\n",
      "-------Fim da epoch nº 1\n",
      "Dados de treino: Loss media da epoch: 0.6890294643478568; f1 medio da epoch: 0.8135241971545576\n",
      "Dados de avaliação: Loss media da epoch: 0.6025674297720537; f1 medio da epoch: 0.8358034092487678\n",
      "FIM DO TREINO! f1 media de treino ao fim de 1 epochs: 0.8135241971545576\n",
      "Treino finalizado para o checkpoint neuralmind/bert-large-portuguese-cased\n",
      "loss_t:[0.6890294643478568], loss_e:[0.6025674297720537], f1_e:[0.8358034092487678], f1_t:[0.8135241971545576]\n",
      "CPU times: user 12min 44s, sys: 2min 26s, total: 15min 11s\n",
      "Wall time: 15min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_pt = compare(portuguese_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c48e32c8-29a9-4232-b600-5d849dea4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt = pd.DataFrame(df_pt)\n",
    "df_pt['checkpoint'] = portuguese_flat\n",
    "df_pt = df_pt.set_index('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e044d690-076e-4995-af36-1df98f1c21b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_t</th>\n",
       "      <th>f1_e</th>\n",
       "      <th>loss_t</th>\n",
       "      <th>loss_e</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checkpoint</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neuralmind/bert-base-portuguese-cased</th>\n",
       "      <td>0.815900</td>\n",
       "      <td>0.839253</td>\n",
       "      <td>0.660471</td>\n",
       "      <td>0.566779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralmind/bert-large-portuguese-cased</th>\n",
       "      <td>0.813524</td>\n",
       "      <td>0.835803</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.602567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            f1_t      f1_e    loss_t    loss_e\n",
       "checkpoint                                                                    \n",
       "neuralmind/bert-base-portuguese-cased   0.815900  0.839253  0.660471  0.566779\n",
       "neuralmind/bert-large-portuguese-cased  0.813524  0.835803  0.689029  0.602567"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20f0684a-7870-4d58-a7f2-ebf3f9336f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt.to_csv('results/checkpoint_pt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a638f0d-dbce-44a3-99c0-faed6103672c",
   "metadata": {},
   "source": [
    "## Comparing english **base** checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb778f-97e7-4a66-b97b-350cb9d3ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progresso: 1/2\n",
      "------Iniciando treino para o checkpoint bert-base-cased---------\n",
      "Tokenizer carregado!\n",
      "Dataloader carregado!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainner carregado!\n",
      "Começando treino! Essa função retorna a media de f1 e loss em cada epoch de treino e avaliação\n",
      "----------Começando treino da epoch nº 1\n",
      "Treinando em cuda\n",
      "Iteração 0 -------- Loss: 2.753157615661621 f1 nas ultimas 100 iterações: 0.0003143912737508796 ------ Progresso: 0.00%.\n",
      "Iteração 100 -------- Loss: 0.2382536083459854 f1 nas ultimas 100 iterações: 0.7871588646859417 ------ Progresso: 10.21%.\n",
      "Iteração 200 -------- Loss: 0.5384090542793274 f1 nas ultimas 100 iterações: 0.8156690559794779 ------ Progresso: 20.43%.\n",
      "Iteração 300 -------- Loss: 0.3716413080692291 f1 nas ultimas 100 iterações: 0.8176875013075072 ------ Progresso: 30.64%.\n",
      "Iteração 400 -------- Loss: 0.3272955119609833 f1 nas ultimas 100 iterações: 0.8075146874991613 ------ Progresso: 40.86%.\n",
      "Iteração 500 -------- Loss: 0.5871379375457764 f1 nas ultimas 100 iterações: 0.8179899213297555 ------ Progresso: 51.07%.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_en = ['bert-base-cased', 'bert-base-uncased']\n",
    "df_en_base = compare(base_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136c1e8-2043-470b-a284-fd5e72d67a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_base = pd.DataFrame(df_en_base)\n",
    "df_en_base['checkpoint'] = base_en\n",
    "df_en_base = df_en_base.set_index('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02695dc-84f2-49b7-97df-f2e947d17322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b706cbd-758c-4748-8cdb-99c0c89f5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_base.to_csv('results/checkpoint_en_base.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1148dc7-5829-47bb-bcfc-dd17b5f779ed",
   "metadata": {},
   "source": [
    "## Comparing english **large** checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cb39260-d657-41d4-b991-d0a1456134e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progresso: 1/2\n",
      "------Iniciando treino para o checkpoint bert-large-cased---------\n",
      "Tokenizer carregado!\n",
      "Dataloader carregado!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainner carregado!\n",
      "Começando treino! Essa função retorna a media de f1 e loss em cada epoch de treino e avaliação\n",
      "----------Começando treino da epoch nº 1\n",
      "Treinando em cuda\n",
      "Iteração 0 -------- Loss: 2.6045541763305664 f1 nas ultimas 100 iterações: 0.00154016374670789 ------ Progresso: 0.00%.\n",
      "Iteração 100 -------- Loss: 0.9199352264404297 f1 nas ultimas 100 iterações: 0.7902711133829589 ------ Progresso: 10.21%.\n",
      "Iteração 200 -------- Loss: 0.6169118881225586 f1 nas ultimas 100 iterações: 0.8072346429979547 ------ Progresso: 20.43%.\n",
      "Iteração 300 -------- Loss: 0.47847306728363037 f1 nas ultimas 100 iterações: 0.8204010252715355 ------ Progresso: 30.64%.\n",
      "Iteração 400 -------- Loss: 0.7637054920196533 f1 nas ultimas 100 iterações: 0.8174800442002979 ------ Progresso: 40.86%.\n",
      "Iteração 500 -------- Loss: 0.34791430830955505 f1 nas ultimas 100 iterações: 0.8089613221079461 ------ Progresso: 51.07%.\n",
      "Iteração 600 -------- Loss: 0.5794242024421692 f1 nas ultimas 100 iterações: 0.805404727318539 ------ Progresso: 61.29%.\n",
      "Iteração 700 -------- Loss: 0.7836333513259888 f1 nas ultimas 100 iterações: 0.8146923336836808 ------ Progresso: 71.50%.\n",
      "Iteração 800 -------- Loss: 0.9748800992965698 f1 nas ultimas 100 iterações: 0.811510781526585 ------ Progresso: 81.72%.\n",
      "Iteração 900 -------- Loss: 0.40923207998275757 f1 nas ultimas 100 iterações: 0.8207379743281487 ------ Progresso: 91.93%.\n",
      "----------Fim do treino. Iniciando avaliação!\n",
      "Iteração 100 -------- Loss: 0.707416832447052 f1 nas ultimas 100 iterações: 0.8457323369976492 ------ Progresso: 57.47%.\n",
      "-------Fim da epoch nº 1\n",
      "Dados de treino: Loss media da epoch: 0.6884654290722997; f1 medio da epoch: 0.8106251879202588\n",
      "Dados de avaliação: Loss media da epoch: 0.5825028639158298; f1 medio da epoch: 0.8370572932077314\n",
      "FIM DO TREINO! f1 media de treino ao fim de 1 epochs: 0.8106251879202588\n",
      "Treino finalizado para o checkpoint bert-large-cased\n",
      "loss_t:[0.6884654290722997], loss_e:[0.5825028639158298], f1_e:[0.8370572932077314], f1_t:[0.8106251879202588]\n",
      "Progresso: 2/2\n",
      "------Iniciando treino para o checkpoint bert-large-uncased---------\n",
      "Tokenizer carregado!\n",
      "Dataloader carregado!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainner carregado!\n",
      "Começando treino! Essa função retorna a media de f1 e loss em cada epoch de treino e avaliação\n",
      "----------Começando treino da epoch nº 1\n",
      "Treinando em cuda\n",
      "Iteração 0 -------- Loss: 3.028632164001465 f1 nas ultimas 100 iterações: 4.8061664984296096e-05 ------ Progresso: 0.00%.\n",
      "Iteração 100 -------- Loss: 0.6114732027053833 f1 nas ultimas 100 iterações: 0.780257454046546 ------ Progresso: 10.21%.\n",
      "Iteração 200 -------- Loss: 0.6320658922195435 f1 nas ultimas 100 iterações: 0.8261782828854299 ------ Progresso: 20.43%.\n",
      "Iteração 300 -------- Loss: 0.40635061264038086 f1 nas ultimas 100 iterações: 0.8087664604218304 ------ Progresso: 30.64%.\n",
      "Iteração 400 -------- Loss: 0.961594820022583 f1 nas ultimas 100 iterações: 0.7978533114933755 ------ Progresso: 40.86%.\n",
      "Iteração 500 -------- Loss: 0.6462196707725525 f1 nas ultimas 100 iterações: 0.8026644734093185 ------ Progresso: 51.07%.\n",
      "Iteração 600 -------- Loss: 0.47248491644859314 f1 nas ultimas 100 iterações: 0.8076144011692294 ------ Progresso: 61.29%.\n",
      "Iteração 700 -------- Loss: 1.2091765403747559 f1 nas ultimas 100 iterações: 0.819063891068275 ------ Progresso: 71.50%.\n",
      "Iteração 800 -------- Loss: 0.6731722950935364 f1 nas ultimas 100 iterações: 0.8288434203140127 ------ Progresso: 81.72%.\n",
      "Iteração 900 -------- Loss: 0.554582953453064 f1 nas ultimas 100 iterações: 0.8058685650119886 ------ Progresso: 91.93%.\n",
      "----------Fim do treino. Iniciando avaliação!\n",
      "Iteração 100 -------- Loss: 0.5275666117668152 f1 nas ultimas 100 iterações: 0.845471699043413 ------ Progresso: 57.47%.\n",
      "-------Fim da epoch nº 1\n",
      "Dados de treino: Loss media da epoch: 0.7052898435673626; f1 medio da epoch: 0.8095146964575551\n",
      "Dados de avaliação: Loss media da epoch: 0.5938833225732563; f1 medio da epoch: 0.8368206102974688\n",
      "FIM DO TREINO! f1 media de treino ao fim de 1 epochs: 0.8095146964575551\n",
      "Treino finalizado para o checkpoint bert-large-uncased\n",
      "loss_t:[0.7052898435673626], loss_e:[0.5938833225732563], f1_e:[0.8368206102974688], f1_t:[0.8095146964575551]\n",
      "CPU times: user 18min 43s, sys: 3min 54s, total: 22min 38s\n",
      "Wall time: 22min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "large_en = ['bert-large-cased', 'bert-large-uncased']\n",
    "df_en_large = compare(large_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a27d2b2-d9b8-46a7-947e-f23377d0b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_large = pd.DataFrame(df_en_large)\n",
    "df_en_large['checkpoint'] = large_en\n",
    "df_en_large = df_en_large.set_index('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44d678ac-7b8f-46bf-a503-fcde53955823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_t</th>\n",
       "      <th>f1_e</th>\n",
       "      <th>loss_t</th>\n",
       "      <th>loss_e</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checkpoint</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert-large-cased</th>\n",
       "      <td>0.810625</td>\n",
       "      <td>0.837057</td>\n",
       "      <td>0.688465</td>\n",
       "      <td>0.582503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-large-uncased</th>\n",
       "      <td>0.809515</td>\n",
       "      <td>0.836821</td>\n",
       "      <td>0.705290</td>\n",
       "      <td>0.593883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        f1_t      f1_e    loss_t    loss_e\n",
       "checkpoint                                                \n",
       "bert-large-cased    0.810625  0.837057  0.688465  0.582503\n",
       "bert-large-uncased  0.809515  0.836821  0.705290  0.593883"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c8a254e-79b8-4667-a269-60d8876c5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_large.to_csv('results/checkpoint_en_large.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fb611-b143-45ca-b0bf-1e3e792a22ea",
   "metadata": {},
   "source": [
    "## Merging the existing dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78cb42c1-fce1-4abc-a33b-b287fc6f247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt = pd.read_csv('results/checkpoint_pt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06706d67-2c4c-4da2-9eaf-fe5071194823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checkpoint</th>\n",
       "      <th>f1_t</th>\n",
       "      <th>f1_e</th>\n",
       "      <th>loss_t</th>\n",
       "      <th>loss_e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neuralmind/bert-base-portuguese-cased</td>\n",
       "      <td>0.815900</td>\n",
       "      <td>0.839253</td>\n",
       "      <td>0.660471</td>\n",
       "      <td>0.566779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neuralmind/bert-large-portuguese-cased</td>\n",
       "      <td>0.813524</td>\n",
       "      <td>0.835803</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.602567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               checkpoint      f1_t      f1_e    loss_t  \\\n",
       "0   neuralmind/bert-base-portuguese-cased  0.815900  0.839253  0.660471   \n",
       "1  neuralmind/bert-large-portuguese-cased  0.813524  0.835803  0.689029   \n",
       "\n",
       "     loss_e  \n",
       "0  0.566779  \n",
       "1  0.602567  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d79396b3-cfbe-42e1-8da0-c5149baebb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_base = pd.read_csv('results/checkpoint_en_base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b7f9a9b-8fe4-4232-b4c3-3a1887b8c4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checkpoint</th>\n",
       "      <th>f1_t</th>\n",
       "      <th>f1_e</th>\n",
       "      <th>loss_t</th>\n",
       "      <th>loss_e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>0.811315</td>\n",
       "      <td>0.839581</td>\n",
       "      <td>0.688456</td>\n",
       "      <td>0.582071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.810601</td>\n",
       "      <td>0.838607</td>\n",
       "      <td>0.687892</td>\n",
       "      <td>0.570408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          checkpoint      f1_t      f1_e    loss_t    loss_e\n",
       "0    bert-base-cased  0.811315  0.839581  0.688456  0.582071\n",
       "1  bert-base-uncased  0.810601  0.838607  0.687892  0.570408"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7e8d124-b84a-4cba-981f-c983852f7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_large = pd.read_csv('results/checkpoint_en_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3d3076e-b23e-44e5-89a0-174f0b700b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checkpoint</th>\n",
       "      <th>f1_t</th>\n",
       "      <th>f1_e</th>\n",
       "      <th>loss_t</th>\n",
       "      <th>loss_e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-large-cased</td>\n",
       "      <td>0.810625</td>\n",
       "      <td>0.837057</td>\n",
       "      <td>0.688465</td>\n",
       "      <td>0.582503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-large-uncased</td>\n",
       "      <td>0.809515</td>\n",
       "      <td>0.836821</td>\n",
       "      <td>0.705290</td>\n",
       "      <td>0.593883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           checkpoint      f1_t      f1_e    loss_t    loss_e\n",
       "0    bert-large-cased  0.810625  0.837057  0.688465  0.582503\n",
       "1  bert-large-uncased  0.809515  0.836821  0.705290  0.593883"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80250b4e-6f21-4de9-9b7b-2e204f38e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_pt, df_en_base, df_en_large])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a41cc093-6615-4812-8cd0-d4ff5be49b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.set_index('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a99c2e29-903e-4dd7-983b-fa5e183e4ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_t</th>\n",
       "      <th>f1_e</th>\n",
       "      <th>loss_t</th>\n",
       "      <th>loss_e</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checkpoint</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neuralmind/bert-base-portuguese-cased</th>\n",
       "      <td>0.815900</td>\n",
       "      <td>0.839253</td>\n",
       "      <td>0.660471</td>\n",
       "      <td>0.566779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralmind/bert-large-portuguese-cased</th>\n",
       "      <td>0.813524</td>\n",
       "      <td>0.835803</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.602567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-base-cased</th>\n",
       "      <td>0.811315</td>\n",
       "      <td>0.839581</td>\n",
       "      <td>0.688456</td>\n",
       "      <td>0.582071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-base-uncased</th>\n",
       "      <td>0.810601</td>\n",
       "      <td>0.838607</td>\n",
       "      <td>0.687892</td>\n",
       "      <td>0.570408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-large-cased</th>\n",
       "      <td>0.810625</td>\n",
       "      <td>0.837057</td>\n",
       "      <td>0.688465</td>\n",
       "      <td>0.582503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-large-uncased</th>\n",
       "      <td>0.809515</td>\n",
       "      <td>0.836821</td>\n",
       "      <td>0.705290</td>\n",
       "      <td>0.593883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            f1_t      f1_e    loss_t    loss_e\n",
       "checkpoint                                                                    \n",
       "neuralmind/bert-base-portuguese-cased   0.815900  0.839253  0.660471  0.566779\n",
       "neuralmind/bert-large-portuguese-cased  0.813524  0.835803  0.689029  0.602567\n",
       "bert-base-cased                         0.811315  0.839581  0.688456  0.582071\n",
       "bert-base-uncased                       0.810601  0.838607  0.687892  0.570408\n",
       "bert-large-cased                        0.810625  0.837057  0.688465  0.582503\n",
       "bert-large-uncased                      0.809515  0.836821  0.705290  0.593883"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc606011-8636-4022-bc48-c92437b9bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"results/all_checkpoints.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
