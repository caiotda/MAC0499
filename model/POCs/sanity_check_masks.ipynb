{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4a4504c6-1e6e-4935-9eb1-e5fda5c156a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4abefd86-8e6d-42df-8bae-b19c5beaebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = 'neuralmind/bert-base-portuguese-cased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ad659283-7fe5-4913-b6ba-8854c1b7c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "64829071-2551-461d-b734-857e71001768",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = [1,1,1, 0]\n",
    "input2 = [2,2,2, 4]\n",
    "batch = [input1, input2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b2dba-ea83-43e5-b2bc-32a35cc7ab0c",
   "metadata": {},
   "source": [
    "Se o attention mask estiver sendo aplicado corretamente, a saída do modelo pro batch deve ser igual ao input com pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8559a0cb-2680-4e55-bbd3-c6f06b65518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_without_pad = input1[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0f15e22e-9ff1-44c4-aa39-979cd29d578e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "90fb9db4-c46e-4b46-b59b-ab92b77e6925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 0], [2, 2, 2, 4]]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e6790857-1dfa-4d30-a1a2-da99c59ee3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 1, 0], [2, 2, 2, 4])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1, input2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72abdf78-b0e8-472b-b072-3258ff2f84c4",
   "metadata": {},
   "source": [
    "Passando dados isoladamente pelo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "099da9f5-9187-426b-b5bd-5200aabd9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = model(torch.tensor([input1_without_pad]))['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5d9507c7-875a-40ba-9e1f-b4ffab852065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3487, -0.1619],\n",
      "         [ 0.1516, -0.1921],\n",
      "         [ 0.2658, -0.1833]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1e50f8e6-5d6d-4f74-9f87-6774da74d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = model(torch.tensor([input2]))['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "82311ca2-e46a-4034-992a-333759f2b3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1650,  0.1033],\n",
       "         [ 0.1337, -0.0854],\n",
       "         [ 0.3468, -0.0913],\n",
       "         [ 0.3077, -0.0307]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b718ac-2428-4935-89be-ee7fc6ffc9a7",
   "metadata": {},
   "source": [
    "Em tese a gente espera que o out3 contendo as duas entradas seja uma aglutinação dos dois, mas não. Por causa do batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "80eedfe5-0691-4433-8b51-43b776c3cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "out3 = model(torch.tensor(batch))['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0fabdaff-38f4-46b1-8b4c-075544fc0399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3736, -0.1760],\n",
       "         [ 0.3407, -0.3460],\n",
       "         [ 0.4535, -0.3298],\n",
       "         [ 0.4237, -0.2578]],\n",
       "\n",
       "        [[ 0.1650,  0.1033],\n",
       "         [ 0.1337, -0.0854],\n",
       "         [ 0.3468, -0.0913],\n",
       "         [ 0.3077, -0.0307]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "dd27ecda-708a-47a1-b665-6d946b199ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([[1,1,1,0],[1,1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f4557185-f4ca-476e-8dce-3c760be6ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "out4 = model(torch.tensor(batch), attention_mask=mask)['logits']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad11d51-ee10-45cf-92c8-9de5c6b890a4",
   "metadata": {},
   "source": [
    "Passando a mascara, o pad é ignorado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6b3b3c63-cb4d-4001-be58-7d9d468d3455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3487, -0.1619],\n",
       "         [ 0.1516, -0.1921],\n",
       "         [ 0.2658, -0.1833],\n",
       "         [ 0.3059, -0.0367]],\n",
       "\n",
       "        [[ 0.1650,  0.1033],\n",
       "         [ 0.1337, -0.0854],\n",
       "         [ 0.3468, -0.0913],\n",
       "         [ 0.3077, -0.0307]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "eeb289de-3002-43cf-bcc4-da2907bb5889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3487, -0.1619],\n",
       "         [ 0.1516, -0.1921],\n",
       "         [ 0.2658, -0.1833]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a47a7147-a8a1-4432-8ff5-7af74fe09c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1650,  0.1033],\n",
       "         [ 0.1337, -0.0854],\n",
       "         [ 0.3468, -0.0913],\n",
       "         [ 0.3077, -0.0307]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df183e9b-a392-43db-932f-afadfab1474a",
   "metadata": {},
   "source": [
    "## Agora a mesma coisa, mas com o código que implementei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4360eeb6-b15b-4c3b-98c5-c4a09018d52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from model import NERClassifier\n",
    "model = NERClassifier(2, CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "041eaad8-8d27-4128-a649-9a5aad29d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = model(torch.tensor([input1_without_pad]), mask=None)['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "53b366b8-fd4d-45b0-80eb-89bf02e8e807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0538, -0.1933],\n",
       "         [-0.0574, -0.1865],\n",
       "         [-0.1432, -0.0405]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cc547cbf-3310-4d55-9dbc-b54f76533300",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = model(torch.tensor([input2]), mask=None)['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "18fe05c6-f240-4fb3-b1b7-32b8ab792593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0109, -0.0509],\n",
       "         [-0.3895,  0.1112],\n",
       "         [-0.5186,  0.1902],\n",
       "         [-0.4196,  0.1357]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0284c3c8-487c-4559-b27d-66181413c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out4 = model(torch.tensor(batch), mask=mask)['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "faa822de-b40d-479e-bcf0-93ad9424524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out5 = model(torch.tensor(batch), mask=None)['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "aa6699c2-4ff1-4540-8a51-fa1618253e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0538, -0.1933],\n",
       "         [-0.0574, -0.1865],\n",
       "         [-0.1432, -0.0405],\n",
       "         [-0.1176, -0.0262]],\n",
       "\n",
       "        [[-0.0109, -0.0509],\n",
       "         [-0.3895,  0.1112],\n",
       "         [-0.5186,  0.1902],\n",
       "         [-0.4196,  0.1357]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6dcf4556-6066-4af6-b143-b440535d951e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0469, -0.1732],\n",
       "         [-0.2796, -0.1695],\n",
       "         [-0.4308, -0.0570],\n",
       "         [-0.4027, -0.1184]],\n",
       "\n",
       "        [[-0.0109, -0.0509],\n",
       "         [-0.3895,  0.1112],\n",
       "         [-0.5186,  0.1902],\n",
       "         [-0.4196,  0.1357]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e09b3-464e-45f7-9f6f-15f64db51d33",
   "metadata": {},
   "source": [
    "Conclusão: meu modelo tá passando a mascara de atenção corretamente. O hugging face até gera uma predição pro padding (o que não faz sentido algum), mas a existencia do padding não altera a previsão para m dado modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
