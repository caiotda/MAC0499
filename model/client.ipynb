{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89390d63-bf24-48f0-9cc0-54df3c5f1367",
   "metadata": {},
   "source": [
    "# Notebook de cliente / Experimentação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0213afb0-1e79-4fff-88e0-1299b29924da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from model import NERClassifier\n",
    "from preprocess_dataset import NERDataset \n",
    "from trainner import Trainner\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d0198-4103-4d77-b5a5-d76616a844bf",
   "metadata": {},
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd363303-49e9-4673-8b3e-de53bcc8869a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lener_br (/home/caiotulio/.cache/huggingface/datasets/lener_br/lener_br/1.0.0/4a8c97e6813b5c2d85a50faf0a3e6c24ea82f4a9044e6e9e8b24997d27399382)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CHECKPOINT = 'neuralmind/bert-base-portuguese-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(CHECKPOINT)\n",
    "data = \"lener_br\"\n",
    "dataset = load_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b20e8238-f4c0-4063-a393-0e5a6a76e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a3cbc51-32d6-44bf-ba40-90a84cd0189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pytorch_dataset_train = NERDataset(data=dataset['train'], max_len=MAX_LEN, tokenizer=tokenizer)\n",
    "teste = pytorch_dataset_train[0]\n",
    "n_labels = 12 #TODO: por enquanto hardcodado \n",
    "model = NERClassifier(n_labels=n_labels, checkpoint=CHECKPOINT)\n",
    "input_ids = teste['input_ids']\n",
    "attention_mask = teste['attention_mask']\n",
    "labels = teste['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639dfd15-a212-498e-835e-5d2a4db979e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd5bc86f-0ff2-4218-9314-a4a00c36ee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  192,  7463,  8427, 22301,   131, 12127,  9008, 22301, 22402, 16484,\n",
      "           187, 22360, 22339,  9008,   118,   177, 22402, 16484, 10836, 13760,\n",
      "          7545, 22320, 22323, 22351, 22301, 22402, 16484,   212,  8718,   250,\n",
      "          7665,  6072,   213,  8718, 22301,  6538,   118, 11635,  9008, 13270,\n",
      "          7073,  6765,   118, 11741, 22328, 22341,  6392, 22301,   212,  9008,\n",
      "         22317,   213,  7073,  6538, 22321, 22352, 21748, 22317,   212, 22371,\n",
      "         22318, 22327,  6162, 22317,   192, 22311,   278,  5650, 22341,   257,\n",
      "          5476, 15289,  5903, 22327,   118,   248, 18199,  6392, 11836, 22309,\n",
      "           118,   177, 10409, 22420, 22320, 14298, 22301, 10836, 13760, 16017,\n",
      "         22322, 22339, 12547, 22402, 16484, 15040, 18868, 22322, 22349, 22341,\n",
      "          9208,   248, 22301, 13760, 11846, 22379, 22320, 14298, 22301,   177,\n",
      "          5226, 22341, 22317,   118, 11635,  3341, 12547, 22402, 22301, 10836,\n",
      "         13760,  4529,  5869, 22351,   118, 11635, 22309, 22333, 22341, 22360,\n",
      "         22351, 22317,   192, 22348,  6538, 16017,  8427, 22309,   118, 11635,\n",
      "          9008, 13270,  7073,  6765, 11247,  7918, 22340,  6392, 22301,   118,\n",
      "           248, 18199,  6392, 11836, 22309,   257,  5476, 12234, 22340,  5476,\n",
      "          6392, 22301,   119,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])\n",
      "torch.Size([1, 256])\n",
      "---------------------\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "torch.Size([1, 256])\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(input_ids.shape)\n",
    "print('---------------------')\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4764938-d372-4943-8889-81a1ddf8ee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenClassifierOutput(loss=tensor(2.5959, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.0262,  0.1934, -0.2084,  ...,  0.4317,  0.1246, -0.0141],\n",
      "         [-0.2335, -0.7291,  0.2143,  ...,  0.5969,  0.4465, -0.0334],\n",
      "         [ 0.2437,  0.0365,  0.0334,  ...,  0.3539,  0.2610, -0.3359],\n",
      "         ...,\n",
      "         [-0.1205,  0.0947, -0.3397,  ...,  0.0253, -0.0181, -0.2614],\n",
      "         [-0.0862,  0.3262, -0.2237,  ...,  0.2073, -0.0428, -0.3868],\n",
      "         [-0.2582,  0.1347, -0.2046,  ...,  0.2651,  0.1042, -0.3297]]],\n",
      "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "saida = model(input_ids, attention_mask, labels=labels)\n",
    "print(saida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9daf3-5b07-481c-8b47-841203c901b8",
   "metadata": {},
   "source": [
    "## Testando o DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27caed91-67b4-4143-99a1-45fc334e7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fcd59ba0-8b8b-4905-86eb-bf6200eb7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd448b-47e5-4bdc-b816-bf7d04f6f646",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4df14cb4-2cea-4b0f-a0b6-b53ef3b89cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<preprocess_dataset.NERDataset at 0x7ff4440e67c0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63cd8df8-7448-457a-b082-b4c01b73f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = iter(DataLoader(pytorch_dataset_train ,batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "146e947c-2e32-4fc7-9155-e7172487dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = next(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8acc3eb-36ef-4b1d-8730-5d4cf474bce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor([0, 1, 2, 3, 4, 5, 6, 7]), 'input_ids': tensor([[[  192,  7463,  8427,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  118,   231,  1328,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  118,  9633,   214,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  354,   119,   189,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  257,  5476, 15289,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  290,   119,     0,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 0,  ..., 0, 0, 0]]]), 'targets': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 9,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print(mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "948e4f05-d0a2-456f-a1a4-10b391eca0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = mini_batch['input_ids'].squeeze().to(\"cuda\", dtype = torch.long) # Remove a dimensão 1 ali do meio, talvez funcione?\n",
    "att = mini_batch['attention_mask'].to(\"cuda\", dtype = torch.long)\n",
    "targets = mini_batch['targets'].to(\"cuda\", dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "58c82c3b-a348-42a4-8d42-81898b046298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1e397-e822-4bda-8a02-5974cc80d752",
   "metadata": {},
   "source": [
    "To tendo o mesmo erro antigo. O modelo reclama do shape do input na hora de fazer o unpack. Ele espera algo como `[batch_size, sequence_length]`. **Resolvido artificalmente usando um squeeze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f51068b6-14d1-4216-a594-82ba7d4b1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model(ids, att, labels=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "09e13bff-406e-40c0-91b8-f8def0d61fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=tensor(2.6167, device='cuda:0', grad_fn=<NllLossBackward>), logits=tensor([[[-1.9685e-02,  2.1916e-01, -6.3078e-02,  ...,  9.1085e-02,\n",
       "           2.3559e-01,  3.2563e-01],\n",
       "         [-3.2816e-01, -5.8023e-01,  2.1085e-01,  ...,  6.3453e-01,\n",
       "           2.8086e-01,  1.8639e-01],\n",
       "         [ 1.1726e-02,  1.3247e-01, -1.4722e-01,  ...,  3.4219e-01,\n",
       "           2.2308e-01, -4.1419e-01],\n",
       "         ...,\n",
       "         [-1.3461e-01,  4.1812e-01,  2.5331e-01,  ...,  2.5084e-01,\n",
       "           4.4213e-01,  9.5656e-02],\n",
       "         [ 7.8919e-02,  6.1588e-01,  1.8569e-01,  ...,  2.2471e-01,\n",
       "           4.9032e-01, -2.0015e-01],\n",
       "         [-2.6598e-01,  1.4384e-01, -3.3398e-01,  ...,  2.8353e-01,\n",
       "           2.6708e-02, -1.1684e-01]],\n",
       "\n",
       "        [[ 9.3541e-02,  6.2980e-01, -3.0648e-02,  ...,  4.2379e-01,\n",
       "           1.1218e-01,  4.5358e-02],\n",
       "         [-3.5411e-01, -2.1601e-01,  2.1364e-03,  ...,  2.3398e-01,\n",
       "          -7.7112e-03, -6.1424e-01],\n",
       "         [-7.3431e-02,  3.3292e-01, -2.1167e-01,  ...,  4.0601e-01,\n",
       "           6.2333e-01, -3.5199e-01],\n",
       "         ...,\n",
       "         [ 4.0712e-02, -2.4952e-01, -1.4362e-01,  ..., -6.0983e-02,\n",
       "          -2.0536e-01, -1.8853e-01],\n",
       "         [-3.3407e-01,  2.8539e-01,  3.9424e-01,  ..., -4.5477e-02,\n",
       "          -2.8678e-01, -2.5110e-01],\n",
       "         [-4.6328e-03,  3.9048e-01,  4.0552e-01,  ..., -1.1076e-01,\n",
       "          -1.1467e-01, -3.1182e-01]],\n",
       "\n",
       "        [[-3.5370e-02,  2.0219e-01,  1.5766e-02,  ...,  4.6079e-01,\n",
       "          -1.3219e-01,  2.4100e-01],\n",
       "         [ 5.0640e-01, -3.6867e-04, -4.2245e-02,  ...,  8.7186e-02,\n",
       "           1.6091e-01, -1.0456e-01],\n",
       "         [ 1.2399e-01,  2.8693e-01,  2.6266e-01,  ...,  3.2838e-01,\n",
       "           1.1615e+00, -3.7989e-01],\n",
       "         ...,\n",
       "         [ 3.3502e-01,  8.8105e-02, -5.2246e-02,  ..., -1.5887e-01,\n",
       "          -1.5324e-01, -1.2650e-01],\n",
       "         [ 2.7017e-01,  2.0046e-01, -2.6915e-01,  ...,  2.7062e-01,\n",
       "          -1.0348e-02,  1.8442e-01],\n",
       "         [ 5.0427e-01,  3.2433e-01, -7.9458e-03,  ...,  1.3659e-02,\n",
       "          -1.3658e-01, -4.3267e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.7041e-01,  2.5926e-01, -2.8079e-01,  ...,  7.8373e-01,\n",
       "           3.0265e-02,  1.5711e-01],\n",
       "         [ 8.0207e-03,  3.5839e-01, -2.9848e-01,  ..., -1.5201e-01,\n",
       "          -3.3642e-01,  1.1356e-02],\n",
       "         [-9.9648e-02,  3.4691e-01, -1.0156e-01,  ..., -7.6269e-02,\n",
       "          -3.7871e-02, -4.0773e-01],\n",
       "         ...,\n",
       "         [-3.5458e-01,  2.6602e-01, -1.3141e-01,  ...,  2.3671e-01,\n",
       "          -1.1331e-01, -5.0571e-01],\n",
       "         [-2.4438e-01,  2.0475e-01, -2.0388e-01,  ...,  4.0849e-01,\n",
       "          -4.3214e-02, -3.3275e-01],\n",
       "         [-9.7177e-02,  8.2669e-03, -2.6519e-01,  ...,  9.5059e-02,\n",
       "           1.8696e-01, -5.0358e-01]],\n",
       "\n",
       "        [[ 1.7816e-01,  3.7721e-01, -3.7312e-02,  ...,  7.2588e-01,\n",
       "          -4.3542e-02,  9.0680e-02],\n",
       "         [-5.8688e-02,  2.5275e-01,  3.8719e-02,  ...,  2.9703e-01,\n",
       "           1.7019e-01, -6.7989e-02],\n",
       "         [-8.7584e-02, -1.5317e-01,  3.2874e-01,  ...,  5.7414e-01,\n",
       "           1.6634e-01, -1.4369e-01],\n",
       "         ...,\n",
       "         [ 1.2183e-01,  2.4357e-01,  2.2107e-03,  ...,  2.0672e-01,\n",
       "          -3.8073e-01, -3.3534e-01],\n",
       "         [-2.7462e-01,  3.9472e-02, -5.9969e-03,  ..., -1.9772e-02,\n",
       "          -1.5951e-01, -1.5746e-01],\n",
       "         [ 4.5882e-02,  1.3648e-01, -9.1537e-02,  ...,  1.2140e-01,\n",
       "          -1.4762e-01, -1.6171e-01]],\n",
       "\n",
       "        [[ 7.6505e-02,  2.0562e-01,  1.4218e-01,  ...,  2.7802e-02,\n",
       "          -1.7226e-01, -1.1727e-01],\n",
       "         [-1.6975e-01,  1.9528e-01,  4.8415e-02,  ..., -1.0020e-01,\n",
       "          -3.5218e-01, -4.0446e-02],\n",
       "         [-5.2203e-02,  2.7842e-01, -9.8129e-02,  ..., -1.5163e-01,\n",
       "          -2.2759e-01, -2.7085e-02],\n",
       "         ...,\n",
       "         [-9.6054e-02,  2.5321e-01, -5.4158e-02,  ..., -2.5811e-01,\n",
       "          -1.6627e-01,  9.1907e-03],\n",
       "         [-4.5353e-02,  1.4391e-01, -1.3912e-01,  ..., -2.0749e-01,\n",
       "          -2.0612e-01, -4.8273e-03],\n",
       "         [ 1.2270e-01,  1.1526e-01,  3.4796e-02,  ..., -1.7484e-01,\n",
       "          -2.1705e-01, -1.0712e-01]]], device='cuda:0', grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fbe6e5-4481-4a29-a7fa-61727996defe",
   "metadata": {},
   "source": [
    "## Testando passar um minibatch ao modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384bf281-ae54-4dd1-8cd1-466e00c6091f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_527151/1991461246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteste\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not an iterator"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ceab436-a34f-4736-8796-2a9b15eff4dc",
   "metadata": {},
   "source": [
    "## Trainando o modelo\n",
    "\n",
    "Usando parametros padrão por enquanto só pra ver se ta tudo indo certindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0914c77b-85a9-420e-b176-5ba779db60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainner import Trainner, create_data_loader\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02601ad0-e5e3-4cf8-9590-376f093eda3e",
   "metadata": {},
   "source": [
    "### Parametros\n",
    "\n",
    "\n",
    "* batch sizes: 8, 16, 32, 64, 128\n",
    "* learning rates: 3e-4, 1e-4, 5e-5, 3e-5\n",
    "\n",
    "Do paper do BERT. Vou colocar num_workers = 4 por enquanto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1dd58-1595-426d-b99f-2217479c4a66",
   "metadata": {},
   "source": [
    "Uma melhoria é esconder o data_loader de clientes. Isso é detalhe de implementaçãõ, ou deveria estar na classe NERDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa9f1459-1272-4515-800b-dac39bd7c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=3e-4, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "358e4323-0b20-41cc-b99e-6cf919520f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7828"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pytorch_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c793179b-78ac-4be4-83c9-f2a9026202ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(pytorch_dataset_train, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dac00bb0-5854-41f8-a14a-ba7b178facc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25a82435-08c3-42c8-b811-0cb5ba2b8e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ddcc80-c1bf-49fa-9464-496ec1aa0c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f771a8fe-dcbb-4a0e-9348-a66ff3a7aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Trainner(device, loader, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea4dd2f2-24af-4778-b918-6cfc6cf9f91e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_527151/1775720147.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MAC0499/model/trainner.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MAC0499/model/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask, labels)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "losses, media = tr._train_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f82a6-3f0c-494f-940f-2e444b1afc37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
